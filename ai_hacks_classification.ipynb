{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMen6TSoREn0chIXP34va3U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AleksanderDanilian/hacksai_construction_technic/blob/master/ai_hacks_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sm7cezuz_Hyh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "from os import listdir\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from collections import Counter\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, WeightedRandomSampler\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchvision.models import ResNet50_Weights, Inception_V3_Weights, EfficientNet_B5_Weights, \\\n",
        "EfficientNet_V2_S_Weights, ResNeXt101_64X4D_Weights\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from joblib import dump, load"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Распаковка архивов с данными\n",
        "\n",
        "!unzip /content/test_dataset_test.zip \n",
        "!unzip /content/train_dataset_train.zip "
      ],
      "metadata": {
        "id": "PkyyRZpm_cUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DIR_TRAIN = \"/content/train/\"\n",
        "DIR_TEST = \"/content/test/\"\n",
        "\n",
        "PATH_TRAIN = \"/content/train.csv\"\n",
        "PATH_TEST = \"/content/test.csv\""
      ],
      "metadata": {
        "id": "hkqlIjam_fyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df = pd.read_csv(PATH_TRAIN)"
      ],
      "metadata": {
        "id": "ENu4xKdK_qd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, data_df, transform=None, test_version=True):\n",
        "\n",
        "        self.data_df = data_df\n",
        "        self.transform = transform\n",
        "        self.test_version = test_version\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # достаем имя изображения и его лейбл\n",
        "        image_name, label = self.data_df.iloc[idx]['ID_img'], self.data_df.iloc[idx]['class']\n",
        "\n",
        "        # читаем картинку. read the image\n",
        "        if not self.test_version:\n",
        "          image = cv2.imread(DIR_TRAIN + f\"{image_name}\")\n",
        "        else:\n",
        "          image = cv2.imread(DIR_TEST + f\"{image_name}\")\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = Image.fromarray(image)\n",
        "        \n",
        "        # преобразуем, если нужно. transform it, if necessary\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        return image, torch.tensor(label).long() \n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data_df)"
      ],
      "metadata": {
        "id": "b_vBu_sd_wae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PretrainedModels():\n",
        "\n",
        "  resnet50_transforms = ResNet50_Weights.DEFAULT.transforms()\n",
        "  inception_v3_transforms = Inception_V3_Weights.DEFAULT.transforms()\n",
        "  efficientnet_b5_transforms = EfficientNet_B5_Weights.DEFAULT.transforms()\n",
        "  resnext_transforms = ResNeXt101_64X4D_Weights.DEFAULT.transforms()\n",
        "\n",
        "  resnet50_weights = ResNet50_Weights.DEFAULT\n",
        "  inception_v3_weights = Inception_V3_Weights.DEFAULT\n",
        "  efficientnet_b5_weights = EfficientNet_B5_Weights.DEFAULT\n",
        "  resnext_weights = ResNeXt101_64X4D_Weights.DEFAULT"
      ],
      "metadata": {
        "id": "VY4gwEd8_zQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# задаем преобразование изображения\n",
        "\n",
        "def build_data_transformers(augmentation = transforms.AutoAugment(), model_transforms = PretrainedModels.resnet50_transforms):\n",
        "  '''\n",
        "  Possible auto augmentations transforms.AutoAugment(), transforms.RandAugment(), transforms.TrivialAugmentWide()\n",
        "  '''\n",
        "\n",
        "  torch.manual_seed(17)  \n",
        "\n",
        "  if augmentation:         \n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        augmentation,\n",
        "        model_transforms\n",
        "    ])\n",
        "  else:\n",
        "    train_transform = transforms.Compose([\n",
        "        model_transforms\n",
        "    ])\n",
        "\n",
        "  valid_transform = transforms.Compose([\n",
        "      model_transforms\n",
        "  ])\n",
        "\n",
        "  return train_transform, valid_transform\n"
      ],
      "metadata": {
        "id": "pxkzibbpANm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# разделим датасет на трейн и валидацию, чтобы смотреть на качество\n",
        "\n",
        "train_df, valid_df = train_test_split(data_df, test_size=0.2, random_state=43, stratify=data_df['class'])\n"
      ],
      "metadata": {
        "id": "FFg-r3qQAhgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# функция подготовки генератора для обучния модели\n",
        "\n",
        "def prepare_loader(model_transforms, model_augmentation, train_df, valid_df, batch_size = 4):\n",
        "  train_transform, valid_transform = build_data_transformers(augmentation = model_augmentation, \n",
        "                                                            model_transforms = model_transforms)\n",
        "  train_dataset = ImageDataset(train_df, train_transform, False)\n",
        "  valid_dataset = ImageDataset(valid_df, valid_transform, False)\n",
        "\n",
        "  # создаем семплер - позволит генератору создавать батчи с равномерным распеределением классов\n",
        "  num_samples = len(train_dataset)\n",
        "  labels = train_dataset.data_df['class'].values\n",
        "  n_classes = len(set(labels))\n",
        "  cnt = Counter(labels)\n",
        "  class_weights = [num_samples/cnt[i] for i in range(n_classes)]\n",
        "  weights = [class_weights[int(labels[i])] for i in range(num_samples)]\n",
        "  sampler = WeightedRandomSampler(weights, len(train_dataset), replacement=True)\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                            batch_size=batch_size,\n",
        "                                            shuffle=False,\n",
        "                                            pin_memory=True,\n",
        "                                            num_workers=2,\n",
        "                                            sampler=sampler)\n",
        "\n",
        "  valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
        "                                            batch_size=batch_size,\n",
        "                                            pin_memory=True,\n",
        "                                            num_workers=2)\n",
        "  \n",
        "  return train_loader, valid_loader\n",
        "\n",
        "def plot_history(train_history, val_history, title='loss'):\n",
        "    plt.figure()\n",
        "    plt.title('{}'.format(title))\n",
        "    dd = list(map(lambda x: x.cpu().detach().numpy(), train_history))\n",
        "    plt.plot(dd, label='train', zorder=1)\n",
        "    \n",
        "    points = np.array(val_history)\n",
        "    steps = list(range(0, len(train_history) + 1, int(len(train_history) / len(val_history))))[1:]\n",
        "\n",
        "    plt.scatter(steps, val_history, marker='+', s=180, c='orange', label='val', zorder=2)\n",
        "    plt.xlabel('train steps')\n",
        "    \n",
        "    plt.legend(loc='best')\n",
        "    plt.grid()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def train(model, criterion, optimizer, scheduler, train_dataloader, test_dataloader, NUM_EPOCH=15, model_type = 'inception', cuda = True):\n",
        "    \n",
        "    epoch_number = 0\n",
        "    train_loss_log = []\n",
        "    val_loss_log = []\n",
        "    \n",
        "    train_acc_log = []\n",
        "    val_acc_log = []\n",
        "\n",
        "    val_loss_min = 1000 # для сохранения лучших весов модели\n",
        "    \n",
        "    for epoch in tqdm(range(NUM_EPOCH)):\n",
        "        epoch_number +=1\n",
        "        model.train()\n",
        "        train_loss = 0.\n",
        "        train_size = 0\n",
        "        \n",
        "        train_pred = 0.\n",
        "\n",
        "        for imgs, labels in train_dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            if cuda:\n",
        "              imgs = imgs.cuda()\n",
        "              labels = labels.cuda()\n",
        "\n",
        "            if model_type == 'inception':\n",
        "              outputs, aux_outputs = model(imgs)\n",
        "              loss1 = criterion(outputs, labels)\n",
        "              loss2 = criterion(aux_outputs, labels)\n",
        "              loss = loss1 + 0.4*loss2\n",
        "              loss.backward()     \n",
        "              train_loss += loss.item()\n",
        "              train_size += outputs.size(0)\n",
        "              train_loss_log.append(loss.data / outputs.size(0))\n",
        "              train_pred += (outputs.argmax(1) == labels).sum()\n",
        "            \n",
        "            else:\n",
        "              y_pred = model(imgs)\n",
        "              loss = criterion(y_pred, labels)\n",
        "              # loss.requires_grad = True\n",
        "              loss.backward() \n",
        "              train_loss += loss.item()\n",
        "              train_size += y_pred.size(0)\n",
        "              train_loss_log.append(loss.data / y_pred.size(0))\n",
        "              train_pred += (y_pred.argmax(1) == labels).sum()\n",
        "            \n",
        "            optimizer.step()\n",
        "\n",
        "        train_acc_log.append(train_pred / train_size)\n",
        "\n",
        "        val_loss = 0.\n",
        "        val_size = 0\n",
        "        \n",
        "        val_pred = 0.\n",
        "        \n",
        "        model.eval()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in test_dataloader:\n",
        "                if cuda:\n",
        "                  imgs = imgs.cuda()\n",
        "                  labels = labels.cuda()\n",
        "                \n",
        "                pred = model(imgs)\n",
        "                loss = criterion(pred, labels)\n",
        "                \n",
        "                val_loss += loss.item()\n",
        "                val_size += pred.size(0)\n",
        "                \n",
        "                val_pred += (pred.argmax(1) == labels).sum()\n",
        "\n",
        "        if val_loss < val_loss_min:\n",
        "          val_loss_min = val_loss\n",
        "          print(val_loss)\n",
        "          torch.save(model.state_dict(), f'/content/{model_type}_model.pth')\n",
        "          print('model_saved. val_loss = ', val_loss_min)\n",
        "          print('epoch number ', epoch_number)\n",
        "        \n",
        "        # Update LR\n",
        "        scheduler.step(val_loss)          \n",
        "        lr_step = optimizer.state_dict()[\"param_groups\"][0][\"lr\"]       \n",
        "\n",
        "        val_loss_log.append(val_loss / val_size)\n",
        "        val_acc_log.append(val_pred / val_size)\n",
        "\n",
        "        # clear_output()\n",
        "        plot_history(train_loss_log, val_loss_log, 'loss')\n",
        "        \n",
        "        print('lr_step', lr_step)\n",
        "        print('Train loss:', (train_loss / train_size)*100)\n",
        "        print('Val loss:', (val_loss / val_size)*100)\n",
        "        print('Train acc:', (train_pred / train_size)*100)\n",
        "        print('Val acc:', (val_pred / val_size)*100)\n",
        "        \n",
        "    return train_loss_log, train_acc_log, val_loss_log, val_acc_log\n",
        "  \n",
        "def get_preds(model, loader):\n",
        "  with torch.no_grad():\n",
        "\n",
        "    all_answers = np.array([])\n",
        "\n",
        "    for imgs, labels in loader:\n",
        "\n",
        "      imgs = imgs.cuda() #\n",
        "      labels = labels.cuda()\n",
        "      \n",
        "      batch_preds = model(imgs).squeeze(1).softmax(1) # get probabilities\n",
        "      batch_preds = batch_preds.cpu().numpy()\n",
        "\n",
        "      if all_answers.shape != (0,):\n",
        "        all_answers = np.vstack((all_answers, batch_preds))\n",
        "      else:\n",
        "        all_answers = batch_preds\n",
        "\n",
        "  print(all_answers.shape)\n",
        "  \n",
        "  return all_answers "
      ],
      "metadata": {
        "id": "Iv2ckinzBDoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(train_df, valid_df, batch_size=16, NUM_EPOCH=25, model_type='inception'):\n",
        "\n",
        "  if model_type == 'efficientnet':\n",
        "    model = models.efficientnet_b5(weights = PretrainedModels.efficientnet_b5_weights)\n",
        "    model.classifier[1] = nn.Linear(2048, 8) \n",
        "    train_loader, valid_loader = prepare_loader(PretrainedModels.efficientnet_b5_transforms, transforms.AutoAugment(), train_df, valid_df, batch_size)\n",
        "  elif model_type == 'inception':\n",
        "    model = models.inception_v3(weights = PretrainedModels.inception_v3_weights)\n",
        "    model.aux_logits=True\n",
        "    model.AuxLogits.fc = nn.Linear(768, 8)\n",
        "    model.fc = nn.Linear(2048, 8)\n",
        "    train_loader, valid_loader = prepare_loader(PretrainedModels.inception_v3_transforms, transforms.AutoAugment(), train_df, valid_df, batch_size)\n",
        "  elif model_type == 'resnet':\n",
        "    model = models.resnet50(weights=PretrainedModels.resnet50_weights)\n",
        "    model.fc = nn.Linear(2048, 8)\n",
        "    train_loader, valid_loader = prepare_loader(PretrainedModels.resnet50_transforms, transforms.RandAugment(), train_df, valid_df, batch_size)\n",
        "  elif model_type == 'resnext':\n",
        "    model = models.resnext101_64x4d(weights = PretrainedModels.resnext_weights)\n",
        "    model.fc = nn.Linear(2048, 8)\n",
        "    train_loader, valid_loader = prepare_loader(PretrainedModels.resnext_transforms, transforms.TrivialAugmentWide(), train_df, valid_df, batch_size)\n",
        "  else:\n",
        "    print('no such model is available')\n",
        "  \n",
        "  model = model.cuda()\n",
        "\n",
        "  criterion = torch.nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "  scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.1, threshold = 0.1, patience=6, verbose = True)\n",
        "  train_loss_log, train_acc_log, val_loss_log, val_acc_log = train(model, criterion, optimizer, scheduler, train_loader, valid_loader, NUM_EPOCH, model_type, cuda = True)\n",
        "\n",
        "  return model, valid_loader\n"
      ],
      "metadata": {
        "id": "PSGlA1bdDEZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RESNET\n",
        "resnet_model, resnet_valid_loader = train_model(train_df, valid_df, batch_size=16, NUM_EPOCH=25, model_type='resnet')\n",
        "resnet_model.eval()\n",
        "\n",
        "# RESNEXT\n",
        "resnext_model, resnext_valid_loader = train_model(train_df, valid_df, batch_size=16, NUM_EPOCH=25, model_type='resnext')\n",
        "resnext_model.eval()\n",
        "\n",
        "# EFFICIENTNET_b5\n",
        "efficientnet_model, efficientnet_valid_loader = train_model(train_df, valid_df, batch_size=16, NUM_EPOCH=25, model_type='efficientnet')\n",
        "efficientnet_model.eval()\n",
        "\n",
        "# INCEPTION_v3\n",
        "inception_model, inception_valid_loader = efficientnet_model = train_model(train_df, valid_df, batch_size=16, NUM_EPOCH=25, model_type='inception')\n",
        "inception_model.eval()"
      ],
      "metadata": {
        "id": "t_WXpfIMIP4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_model"
      ],
      "metadata": {
        "id": "9p9whjwuZ6QD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# создаем даталоадеры для каждой модели отдельно (т.к. обучаем xgboost classifier - берем валидационные данные, которые наши нейронки не видели.)\n",
        "\n",
        "efficientnet_preds = get_preds(efficientnet_model[0], efficientnet_valid_loader)\n",
        "resnet_preds = get_preds(resnet_model, resnet_valid_loader)\n",
        "inception_preds = get_preds(inception_model, inception_valid_loader)\n",
        "resnext_preds = get_preds(resnext_model, resnext_valid_loader)\n",
        "\n",
        "all_models_preds = np.concatenate((resnet_preds, inception_preds, resnext_preds, efficientnet_preds), 1)\n",
        "valid_labels = valid_df['class'].values.astype(int)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bugq26MWJJaX",
        "outputId": "80c44dde-c951-4729-f68d-ec4b302b57da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(998, 8)\n",
            "(998, 8)\n",
            "(998, 8)\n",
            "(998, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test = all_models_preds[:700], all_models_preds[700:]\n",
        "y_train, y_test = valid_labels[:700], valid_labels[700:]\n",
        "\n",
        "clf = GradientBoostingClassifier(n_estimators=2500, learning_rate=0.01, max_depth=1, random_state=0, subsample = 1).fit(X_train, y_train)\n",
        "print(clf.score(X_test, y_test))\n",
        "\n",
        "dump(clf, '/content/xgboost_model.joblib') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tR4DVr-cKsd4",
        "outputId": "7d0f16df-8ab7-4658-bdef-c951fa116172"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.785234899328859\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/xgboost_model.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# проверяем наш xgboost классификатор на тестовом наборе данных\n",
        "\n",
        "test_df = pd.read_csv(PATH_TEST)\n",
        "\n",
        "resnet_dataset = ImageDataset(test_df, PretrainedModels.resnet50_transforms, True)\n",
        "resnet_loader = torch.utils.data.DataLoader(dataset=resnet_dataset,\n",
        "                                           batch_size=16,\n",
        "                                           pin_memory=True,\n",
        "                                           num_workers=2)\n",
        "\n",
        "inception_dataset = ImageDataset(test_df, PretrainedModels.inception_v3_transforms, True)\n",
        "inception_loader = torch.utils.data.DataLoader(dataset=inception_dataset,\n",
        "                                           batch_size=16,\n",
        "                                           pin_memory=True,\n",
        "                                           num_workers=2)\n",
        "\n",
        "resnext_dataset = ImageDataset(test_df, PretrainedModels.resnext_transforms, True)\n",
        "resnext_loader = torch.utils.data.DataLoader(dataset=resnext_dataset,\n",
        "                                           batch_size=16,\n",
        "                                           pin_memory=True,\n",
        "                                           num_workers=2)\n",
        "\n",
        "efficientnet_dataset = ImageDataset(test_df, PretrainedModels.efficientnet_b5_transforms, True)\n",
        "efficientnet_loader = torch.utils.data.DataLoader(dataset=efficientnet_dataset,\n",
        "                                           batch_size=16,\n",
        "                                           pin_memory=True,\n",
        "                                           num_workers=2)\n",
        "\n",
        "\n",
        "efficientnet_test_preds = get_preds(efficientnet_model[0], efficientnet_loader)\n",
        "resnet_test_preds = get_preds(resnet_model, resnet_loader)\n",
        "inception_test_preds = get_preds(inception_model, inception_loader)\n",
        "resnext_test_preds = get_preds(resnext_model, resnext_loader)\n",
        "\n",
        "all_models_test_preds = np.concatenate((resnet_test_preds, inception_test_preds, resnext_test_preds, efficientnet_test_preds), 1)\n",
        "\n",
        "xgboost_predicts = clf.predict(all_models_test_preds)\n",
        "test_df = test_df.drop([\"class\"], axis = 1)\n",
        "test_df[\"class\"] = xgboost_predicts\n",
        "\n",
        "test_df.to_csv(\"/content/submit.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDH9aEMkLLPz",
        "outputId": "156b90f6-4b6d-477e-899e-4458792940fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2138, 8)\n",
            "(2138, 8)\n"
          ]
        }
      ]
    }
  ]
}